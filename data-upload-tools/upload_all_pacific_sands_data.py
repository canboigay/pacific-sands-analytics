#!/usr/bin/env python3
'''
Upload ALL Pacific Sands historical data from CSV and Excel files
'''
import os
import pandas as pd
import requests
import json
from datetime import datetime
import glob

print('üåä PACIFIC SANDS DATA UPLOADER')
print('=' * 50)

# Configuration
API_URL = 'http://localhost:3000/api/upload'  # Use local API first
BACKUP_API = 'https://pacific-sands-api.vercel.app/api/upload'
API_KEY = 'ps_me2w0k3e_x81fsv0yz3k'

headers = {
    'Authorization': f'Bearer {API_KEY}',
    'Content-Type': 'application/json'
}

# Find all CSV and Excel files
def find_data_files():
    '''Find all Pacific Sands data files'''
    print('\nüìÇ Searching for data files...')
    
    files = []
    
    # Search patterns
    search_dirs = [
        os.path.expanduser('~/Downloads'),
        os.path.expanduser('~/Desktop'),
        os.path.expanduser('~/Documents'),
        os.path.expanduser('~/pacific-sands-analytics'),
        os.path.expanduser('~/pacific-sands-analytics/Downloads/PacificSands_AllCSV')
    ]
    
    for directory in search_dirs:
        if os.path.exists(directory):
            # Find CSV files
            csv_files = glob.glob(os.path.join(directory, '*.csv'))
            files.extend(csv_files)
            
            # Find Excel files
            xlsx_files = glob.glob(os.path.join(directory, '*.xlsx'))
            files.extend(xlsx_files)
            
            xls_files = glob.glob(os.path.join(directory, '*.xls'))
            files.extend(xls_files)
    
    # Filter for Pacific Sands related files
    pacific_files = []
    keywords = ['pacific', 'sands', 'rate', 'occupancy', 'revenue', 'booking']
    
    for f in files:
        filename = os.path.basename(f).lower()
        if any(keyword in filename for keyword in keywords):
            pacific_files.append(f)
    
    print(f'Found {len(pacific_files)} Pacific Sands data files')
    return pacific_files

# Read and process files
def process_file(filepath):
    '''Process a single data file'''
    print(f'\nüìÑ Processing: {os.path.basename(filepath)}')
    
    try:
        # Read file based on extension
        if filepath.endswith('.csv'):
            df = pd.read_csv(filepath)
        elif filepath.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(filepath)
        else:
            return None
        
        print(f'  Rows: {len(df)}, Columns: {list(df.columns)}')
        
        # Detect data type based on columns
        columns_lower = [col.lower() for col in df.columns]
        
        data_type = 'rates'  # default
        if any('competitor' in col for col in columns_lower):
            data_type = 'competitors'
        elif any('review' in col or 'rating' in col for col in columns_lower):
            data_type = 'feedback'
        elif any('rate' in col or 'price' in col for col in columns_lower):
            data_type = 'rates'
        
        # Process based on data type
        formatted_data = []
        
        for _, row in df.iterrows():
            try:
                if data_type == 'rates':
                    # Look for rate/price columns
                    rate_val = None
                    for col in df.columns:
                        if 'rate' in col.lower() or 'price' in col.lower():
                            rate_val = float(row[col]) if pd.notna(row[col]) else None
                            break
                    
                    if rate_val:
                        formatted_data.append({
                            'date': str(row.get('date', datetime.now().isoformat())),
                            'rate': rate_val,
                            'room_type': str(row.get('room_type', 'Standard')),
                            'occupancy': float(row.get('occupancy', 0.75)) if 'occupancy' in df.columns else 0.75
                        })
                
                elif data_type == 'competitors':
                    formatted_data.append({
                        'competitor': str(row.get('competitor', 'Unknown')),
                        'date': str(row.get('date', datetime.now().isoformat())),
                        'rate': float(row.get('rate', row.get('price', 250))),
                        'room_type': str(row.get('room_type', 'Standard'))
                    })
                
                elif data_type == 'feedback':
                    formatted_data.append({
                        'date': str(row.get('date', datetime.now().isoformat())),
                        'rating': float(row.get('rating', 4.0)),
                        'comment': str(row.get('comment', row.get('review', ''))),
                        'category': str(row.get('category', 'General'))
                    })
                    
            except Exception as e:
                continue  # Skip problematic rows
        
        if formatted_data:
            print(f'  ‚úÖ Processed {len(formatted_data)} records of type: {data_type}')
            return {'data_type': data_type, 'data': formatted_data, 'filename': os.path.basename(filepath)}
        else:
            print(f'  ‚ö†Ô∏è  No valid data extracted')
            return None
            
    except Exception as e:
        print(f'  ‚ùå Error reading file: {e}')
        return None

# Upload data to API
def upload_to_api(payload):
    '''Upload processed data to API'''
    if not payload or not payload['data']:
        return False
    
    print(f'  üì§ Uploading {len(payload["data"])} {payload["data_type"]} records...')
    
    # Try local API first
    try:
        response = requests.post(API_URL, json=payload, headers=headers, timeout=5)
        if response.status_code == 200:
            print(f'  ‚úÖ Successfully uploaded to local API')
            return True
    except:
        pass
    
    # Try production API
    try:
        response = requests.post(BACKUP_API, json=payload, headers=headers, timeout=10)
        if response.status_code == 200:
            print(f'  ‚úÖ Successfully uploaded to production API')
            return True
        else:
            print(f'  ‚ùå Upload failed: {response.status_code}')
    except Exception as e:
        print(f'  ‚ùå API error: {e}')
    
    return False

# Main execution
def main():
    # Find all data files
    files = find_data_files()
    
    if not files:
        print('\n‚ö†Ô∏è  No Pacific Sands data files found!')
        print('Please check that your CSV/Excel files are in:')
        print('  - Downloads folder')
        print('  - Desktop folder')
        print('  - Documents folder')
        return
    
    print(f'\nüìä Ready to process {len(files)} files')
    
    total_uploaded = 0
    
    for filepath in files:
        payload = process_file(filepath)
        if payload:
            # Upload in batches of 100 records
            data = payload['data']
            for i in range(0, len(data), 100):
                batch = data[i:i+100]
                batch_payload = {
                    'data_type': payload['data_type'],
                    'data': batch,
                    'filename': payload['filename']
                }
                if upload_to_api(batch_payload):
                    total_uploaded += len(batch)
    
    print('\n' + '=' * 50)
    print(f'‚úÖ UPLOAD COMPLETE!')
    print(f'üìä Total records uploaded: {total_uploaded}')
    print(f'\nüéØ Your dashboard should now show REAL DATA!')
    print('   Visit: http://localhost:3000/live-dashboard')

if __name__ == '__main__':
    main()
